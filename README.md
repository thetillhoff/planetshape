# `planetshape`/`dust`

## The vision
Generic system for most common components that everyone can use. Cloud or not. Knowing tech details or not. Having product preferences or not. Sensible defaults for everyone, obscuring unnecessary details. Provider of plugin must ensure support of the plugin.

## Preparation problems
- WE NEED ORDERED LISTS FOR THIS! It solves all our problems! We can use dictionaries with numbers as keys to get that.
  We can then try to match stuff we already have with our 'schema' and find things that don't match anything. Could be a hassle to implement though...
- cores & ghz could be auto-converted to manufacturer and model no -> help the user finding them

# The ideas

- Similar to terraform but more abstract.
- Might 'steal' ideas from pulumi and tosca.
- Assumption: Every major provider provides an API to consume/manage its services.
  It should be relatively easy to write wrappers around those. These can then behave similar to terraform providers.
- Own typesystem, similar to tosca. less 'duplicates' though. Allow abstracts/derivation, one type of attributes, substitution (tf does have this as well but worse).
  - Start bottom-up: Compute, Storage, Network first. Each layer has one uniform abstract typeset - provider modules implement those, but not necessarily all types of a layer.
    The abstract types are the only ones that will be used / supported
  - Provide additional components like databases, user mgmt/auth, load balancers, DNS/nameservers etc on top of this lower level ('consumes compute'...)
  - Abstract base as well, which uses the reference implementation - users can override this with their own implementation. Maybe use a subset of the CNCF landscape for this?
  - This can then be used to create recommendation for performance and prices (product XY is cheaper when taking your use case of Z into account (user-number, performance requirements, ...))
- api/provider/plugin system like terraform; rpc, simplified tosca typesystem, library capability like pulumi
- each abstract component has a defined api for input and output that is equal by all providers. No additional settings are allowed for uniformity reasons.
- state either like terraform - ther ehas to be asystem that checks whether a resource exists. Alternative: projectwide unique names, labels, annotation that can be used instead. Example: create hash of all attributes or an object-id that is generated in a repeatable way.
- Dependencies are resolved in plan phase - like in terraform. -> no circular dependencies, ...
- An object can depend on the first of the list, but not the rest -> this allows for master-slave setups like for kubeadm, databases, ...
- plan phase includes verifying if all provider plugins are in place, all required api calls exist / are implemented by testing them with dummy values (--dry-run or similar?)
- cost estimation can be exposed via api as well, but is optional. core module has to show result and which costs are unkown.
- core module handles communication and plugin calls only, no auth, no types - except abstract ones.
- plugin updates are done by adjusting version - might autodetect by loading versionlist and print info
- core update works equally, one minimal binary that only manages downloads is used as entry, it then detects used api version in entry manifest, then calls teh respective core version, which then calls its required plugins (shared plugin by all core versions)
- if minimal core needs an update, users need to do it themselves
- advanced services like eks, alb can be used if overriden in plugin (as described before) - could be recommended by cost estimation tool.
- based on this tool, another could analyse performance history and recommend other things like different providers, tools, specific settings like disk size/settings, bigger/smaller/more/less nodes ...
- add migration plans for common software like wordpress, docker swarm, eks, ...
- highest level of abstraction: ECS 'clone' == clustered container orchestrator
- providers for databases/storage need to include means to access data, f.e. for snapshots, backup, restore, download, upload - via their plugin (can include additional binaries like mysqldump)
- references to other components can be made in terraform manner, apis are already known before runtime
- plan phase should include price estimation and recommendation
- plugins can download additional files like mysqldump (only in their respective folder and in one particular version) automatically or ask the user to install specific tools like aws-ctl into the path or similar. frowned upon, but might be necessary
- if users want to upgrade their planetshape/dust/plugin-version, they can add a flag to their plan call `--upgrade` which updates the currently max supported version in the current manifests and tells the user if there are additional migration steps in order to support the newest versions - generated by rpc calls of the plugins, f.e. validate with the next-highest version
- planetshape/dust saves the timestamp of the last call to each module incl. version. that way, it can recommend to delete older/unused ones - either manually or with a `clean` command (`--before 2020-01-01`, `--before 2020-01`, `--before 2020`)
- each plugin has an auth endpoint that might require specific inputs. they can be provided via env vars or env files. they can also be scripted (by each provider). examples are hcloud user id and token, cloudfront project id and token. gh actions etc support secret env vars - as do most pipeline tools. auth should be verified one plugin after another, so if the script calls a website wait until the user is finished before starting the next. tokens should be used in most cases, as pipelines cant use website login. env files would be in .dust folder or similar. profiles possible, default is `default`. can be empty
- `--prune` or `--purge` removes unreferenced stuff from the providers. for example ec2 instances that were deleted from code. can be combined with `--plan`.
- `--plan` has two stages: first verify locally, with dependencies, etc. Second stage gets state, either from stored state like terraform, or accesses providers; can verify even more this way like duplicate DNS entries
- state can be temorary if naming schema for resources is well-defined (same resource results in same name). maybe has objects in list?
  changed resource leads to recreation
- configuration drift could be detected this way, but might be rather complex. Maybe every plugin has to set default values and verify them when checking existing resources?
- reconcilation is basically a cronjob for running dust/planetshape. can be run for specific subcomponent only as well.
- how to include gitops / kubernetes cicd? Or how to deploy k8s manifests? Bootstrapping the cluster might be sufficient, everything else is left for other tools.
  Or implement a kubernetes api wrapper ;)
- provider / service documentation can state things like: can only be embedded in a (generic) kubernetes cluster. might allow 'in an AWS kubernetes cluster of type 'EKS' when ec2 cluster is not sufficient.
- maybe do not allow dependencies. only allow modules to include others. that way, whenever something is changed the necessary stuff is updated, but it stays idempotent

